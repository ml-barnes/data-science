{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data from the web\n",
    "\n",
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is about automating the extraction of information from a website or lists of websites (simulating a human searching for data on the web, copying the data that matched a prespecified pattern and pasting that data into a database table $X$) by finding patterns in the website source code (usually HTML). The general idea behind web scraping is to retrieve data that exists on a website, and convert it into a format that is usable for analysis, usually the feature matrix $X$ you all are already familiar with. \n",
    "\n",
    "Webpages are rendered by the brower from HTML and CSS code, but much of the information included in the HTML underlying any website is not interesting to us. Hence, we need to use tools that allows us to fetch specific pieces of information.\n",
    "\n",
    "If you want to do some serious web scrapping, keep in mind that:\n",
    "\n",
    "- Scraping too many pages too fast can get your IP address blocked\n",
    "- Pay attention to the robots exclusion standard (`robots.txt`) of the website you want to scrape\n",
    "- Look at http://www.imdb.com/robots.txt for an example of a `robots.txt` file\n",
    "![robot](./images/robot.jpg)\n",
    "\n",
    "Keep in mind also that HTML is a markup language interpreted by a web browser to produce (\"render\") a webpage. Such elements are opened with an opening tag and need to be closed with a closing tag. Tags can also have optional attributes. \n",
    "\n",
    "Recall also that a webpage it is abstractly represented by the browser as a tree, the Document Object Model (DOM):\n",
    "\n",
    "![](./images/dom.png)\n",
    "\n",
    "A modern tool for efficient scraping of information contained in a single or across several web domains is [Scrapy](https://scrapy.org). For this practical you are asked to follow the Scrapy tutorial to learn how to use the framework:\n",
    "https://docs.scrapy.org/en/latest/intro/tutorial.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
