{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Watson Visual Recognition service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Service Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watson Visual Recognition service is a Service for which we need specific credentials (you will need api specific credentials for each Watson service). These credentials are different from your Bluemix account username and password.\n",
    "\n",
    "To get your Watson visual recognition service credentials, follow these steps:\n",
    "\n",
    "1. Log in to Bluemix at https://bluemix.net.\n",
    "\n",
    "2. Go to: https://cloud.ibm.com/developer/watson/services\n",
    "\n",
    "3. Create an instance of the **Lite** Visual Recognition service:\n",
    "\n",
    "4. Click the `Create` button (You have now created a cloud service for visual recognition which you should be able to view on your Watson dashboard)\n",
    "\n",
    "5. Open the service and click on the \"Service Credentials\" link to view your service credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start test the service we have created by classifying the following images:\n",
    "\n",
    "![](./resources/beach.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "![](./resources/dog.jpg)\n",
    "\n",
    "----\n",
    "\n",
    "![](./resources/train.jpg)\n",
    "\n",
    "----\n",
    "\n",
    "![](./resources/joy.jpg)\n",
    "\n",
    "In the following code snippet you need to inject into the `api_key` parameter your own api_key found in the Service credentials of your Watson Visual recognition service.\n",
    "\n",
    "Uncomment the `test_url` variables one at a time and study the information that Watson visual recognition `classify` method returns each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drozado\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: watson-developer-cloud moved to ibm-watson. To get updates, use the new package.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"beach\",\n",
      "              \"score\": 0.85,\n",
      "              \"type_hierarchy\": \"/nature/beach\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"nature\",\n",
      "              \"score\": 0.982\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"atoll\",\n",
      "              \"score\": 0.665,\n",
      "              \"type_hierarchy\": \"/nature/ridge/atoll\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"ridge\",\n",
      "              \"score\": 0.665\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"strand\",\n",
      "              \"score\": 0.64,\n",
      "              \"type_hierarchy\": \"/nature/shore/strand\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"shore\",\n",
      "              \"score\": 0.796\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"seashore\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/nature/shore/seashore\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"natural elevation\",\n",
      "              \"score\": 0.801\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"tree\",\n",
      "              \"score\": 0.763\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"sea green color\",\n",
      "              \"score\": 0.984\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"beach.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join, dirname\n",
    "from os import environ\n",
    "from watson_developer_cloud import VisualRecognitionV3\n",
    "# from ibm_watson import VisualRecognitionV3\n",
    "\n",
    "#beach\n",
    "test_url = './resources/beach.jpg' \n",
    "\n",
    "#dog\n",
    "# test_url = './resources/dog.jpg' \n",
    "\n",
    "#train\n",
    "# test_url = './resources/train.jpg' \n",
    "\n",
    "#Joy\n",
    "# test_url = './resources/joy.jpg' \n",
    "\n",
    "#Below you need to substitute your own API_key1\n",
    "visual_recognition = VisualRecognitionV3(version='2018-03-19',iam_apikey='')\n",
    "\n",
    "with open(test_url, 'rb') as images_file:\n",
    "    classes = visual_recognition.classify(images_file).get_result()\n",
    "    print(json.dumps(classes, indent=2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move to the ability of Watson to recognize faces in images and even specific facial features such as age or gender. Once again, uncomment the `test_url` variables one at a time and study the information that Watson visual recognition `detect_faces` method returns each time for the following faces:\n",
    "\n",
    "![](./resources/joy.jpg)\n",
    "![](./resources/tom.jpg)\n",
    "![](./resources/david.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 51,\n",
      "            \"max\": 55,\n",
      "            \"score\": 0.7089185\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 66,\n",
      "            \"width\": 61,\n",
      "            \"left\": 50,\n",
      "            \"top\": 40\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"gender_label\": \"female\",\n",
      "            \"score\": 0.990677\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"joy.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_url = './resources/joy.jpg' \n",
    "# test_url = './resources/tom.jpg' \n",
    "# test_url = './resources/david.jpg' \n",
    "\n",
    "with open(test_url, 'rb') as images_file:\n",
    "    classes = visual_recognition.detect_faces(images_file).get_result()\n",
    "    print(json.dumps(classes, indent=2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not perfect predictions, the accuracy of the system to detect gender and age estimations is pretty impressive."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
