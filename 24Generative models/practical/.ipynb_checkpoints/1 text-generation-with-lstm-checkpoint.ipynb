{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with LSTMs\n",
    "\n",
    "## Implementing character-level LSTM text generation\n",
    "\n",
    "\n",
    "Let's use an LSTM neural network model to generate text using the writing style from a reference book. The first thing we need is a lot of text data that we can use to learn a language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. \n",
    "\n",
    "In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English) or the Bible (your choice). The language model we will learn will thus be specifically a model of Nietzsche's writing style or the Bible writing style rather than a more generic model of the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's start by converting the corpus to lowercase. We also only look at the first 600,000 characters of the text in order for the training to not be too slow. In the following code snippet choose whether you want to create a model from the Bible or from Nietzsche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600000\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path=\"Bible.txt\"\n",
    "#path=\"nietzsche.txt\"\n",
    "\n",
    "text = open(path).read().lower()\n",
    "text = text[:600000]\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will extract partially-overlapping sequences of length `maxlen`, one-hot encode them and pack them in a 3D Numpy array `x` of  shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare an array `y` containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 199980\n",
      "Unique characters: 55\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language models\n",
    "\n",
    "The universal way to generate sequence data in deep learning is to train a network (usually an RNN or a convnet) to predict the next token or next few tokens in a sequence, using the previous tokens as input. For instance, given the input “the cat is on the ma,” the network is trained to predict the target t, the next character. As usual when working with text data, tokens are typically words or characters, and any network that can model the probability of the next token given the previous ones is called a language model. \n",
    "\n",
    "A language model captures the latent space of language: its statistical structure. Once you have such a trained language model, you can sample from it (generate new sequences): you feed it an initial string of text (called conditioning data), ask it to\n",
    "generate the next character or the next word (you can even generate several tokens at once), add the generated output back to the input data, and repeat the process many times. \n",
    "\n",
    "![](./images/lm.png)\n",
    "\n",
    "This loop allows you to generate sequences of arbitrary length that reflect the structure of the data on which the model was trained: sequences that look almost like human-written sentences. In the example we present in this section, you’ll take a LSTM layer, feed it strings of N characters extracted from a text corpus, and train it to predict character N + 1. The output of the model will be a softmax over all possible characters: a probability distribution for the next character. This LSTM is\n",
    "called a character-level neural language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Our network is a single `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\droza\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The temperature parameter\n",
    "In order to control the amount of stochasticity in the sampling process, we’ll introduce a parameter called the softmax temperature that characterizes the entropy of the probability distribution used for sampling: it characterizes how surprising or predictable the choice of the next character will be. Given a temperature value, a new probability distribution is computed from the original one (the softmax output of the model) by reweighting it.\n",
    "\n",
    "Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data\n",
    "\n",
    "![](./images/t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the language model and sampling from it\n",
    "\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "\n",
    "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "* 2) Reweighting the distribution to a certain \"temperature\"\n",
    "* 3) Sampling the next character at random according to the reweighted distribution\n",
    "* 4) Adding the new character at the end of the available text\n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, \n",
    "and draw a character index from it (the \"sampling function\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of temperature in the sampling strategy.\n",
    "\n",
    "To generate strong results, you should go up to 60 training Epochs. However, to finish training within the allotted classroom time, in the following code snippet I only train up to 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "199980/199980 [==============================] - 275s 1ms/step - loss: 1.6532\n",
      "--- Generating with seed: \"8 and i will give unto thee, and to thy seed after thee, the\"\n",
      "------ temperature: 0.2\n",
      "8 and i will give unto thee, and to thy seed after thee, the lord shall be the father and them them them, and he shall be had had shall be shall be beast them and the sons, and the sons of the father, and he shall be them, and the lord had beast them them them them them, and he shall be shall be the father and the father of the land.\n",
      "\n",
      "1:14 and he shall be the lord.\n",
      "\n",
      "1:26 and he shall be the thing of the father and her the hand of the land.\n",
      "\n",
      "10:22 and he sh\n",
      "------ temperature: 0.5\n",
      "of the father and her the hand of the land.\n",
      "\n",
      "10:22 and he shall the house of him them.\n",
      "\n",
      "20:16 the stranger of the camp of the father had beast the land.\n",
      "\n",
      "2:16 and hasand him after them it stren them of his father of him of them in the fire of the father, and the rame of the tabernacle\n",
      "of them, that is them of them, and the tabernacle of the eating that israel of the tabernacle of them, that they shall be upon him them thee, it is a stranger in them, dused \n",
      "------ temperature: 1.0\n",
      "hall be upon him them thee, it is a stranger in them, dused his edy by egypt to hahalse, much in\n",
      "their  make ittret.\n",
      "\n",
      "22:1 and his one with all the lang, and which nows.\n",
      "\n",
      "21:34 he niven them, that dodan's ownaring of gaepainoniy, and hered them\n",
      "urth: for the saidiceit:\n",
      "feat that jabone\n",
      "had mise his wat in fe my brash and the lord.\n",
      "\n",
      "2:22 and the wordsoo them of chauderneded from you, and iedat.\n",
      "\n",
      "19:15 and he shall unto your\n",
      "sight; and as befoce,\n",
      "by it by th\n",
      "------ temperature: 1.2\n",
      ":15 and he shall unto your\n",
      "sight; and as befoce,\n",
      "by it by thitveb my congregation,\n",
      "25:19 is mathes them shall the\n",
      "camp one now anth, afty\n",
      "a make trewners, ther hasred? and the man shall that it laaateng the rmaesen shall,\n",
      "and oy bullocch,\n",
      "and\n",
      "hils wind;.\n",
      "o0, osefters, i withod them.\n",
      "a9 dofper, upon him: and her fem, it\n",
      "back, and the younger, ye whet\n",
      "they my from\n",
      "shall ed shamobeid, abighade land.\n",
      "\n",
      "14:13 i fell giddlood, them\n",
      "eat aby anetihb, and\n",
      "hecard, fr\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "199980/199980 [==============================] - 279s 1ms/step - loss: 1.2929\n",
      "--- Generating with seed: \"45:13 and ye shall tell my father of all my glory in egypt, \"\n",
      "------ temperature: 0.2\n",
      "45:13 and ye shall tell my father of all my glory in egypt, which the sons of the sons of the sons of the sons of the sons of the lord.\n",
      "\n",
      "15:2 then the seven days of the sons of the seven day the sons which the sons of the sons of the sons of the sons of the congregation, and the lord spake unto moses, and he shall be a son of the sons of the sons of the sons of the sons of the sons of the sons of the seven days, and the sons of the sons of the sons of the \n",
      "------ temperature: 0.5\n",
      "the seven days, and the sons of the sons of the sons of the sons of the wives, and the seven son of the goats of the father, and sockets unto moses, and made the people that were to the sons of aaron and all the sons which rester them, and seven days, and was from the father of the send which they be to day and said, and twenty died of the flocks of the congregation,\n",
      "the sons which they be not from them, and surely the morn the offering went unto him, to m\n",
      "------ temperature: 1.0\n",
      "m them, and surely the morn the offering went unto him, to may be a dlord in to lown unto him, when now ye shall eat it that repaten that he be? and he dwell the fruit\n",
      "of the cabout.\n",
      "\n",
      "20:14 and if it be an tomed three that isell the\n",
      "callarth, after the fidly shall be wleans, it she gut the goroumons, from moses, to jacobai the peventitel\n",
      "in hath, and bride us swall f the pessels were me, i pervants, wely not called my sing; and they be not shall fire, and \n",
      "------ temperature: 1.2\n",
      "s, wely not called my sing; and they be not shall fire, and well yearams, 4\n",
      "be israel,\n",
      "forolly unto his leving, beil, that which , and sacrifice.\n",
      "\n",
      "11:8 son cremadf, im,\n",
      "whiches father flesh, wror funt: and he broat\n",
      "thou wefy wit whee sto heach unclean to fold tockem: even.\n",
      "\n",
      "9emont shavl yearedred wo od mownt side afferien them\n",
      "of then, neither wemeld unto the egyptian.\n",
      "\n",
      "6:11 and thou alto the broth of oils in to his\n",
      "\n",
      "where stonem, 322:25 and\n",
      "be, som, and n\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "199980/199980 [==============================] - 266s 1ms/step - loss: 1.2130\n",
      "--- Generating with seed: \" likeness:\n",
      "and let them have dominion over the fish of the s\"\n",
      "------ temperature: 0.2\n",
      " likeness:\n",
      "and let them have dominion over the fish of the second of the congregation the people there of the children of israel, and the court shall be a son of the children of israel, and the people shall the plague of the congregation, and the lord begat the lord had he have say unto the lord, and the plague of the service of the lord god all the lord your god, and the plague of the lord god saying, the people that the lord spake unto moses, saying, and\n",
      "------ temperature: 0.5\n",
      "ying, the people that the lord spake unto moses, saying, and he shall eat the son of the lord have his servants to the children of israel, and of the cloud in the people thereof, and the lord chared his wive in the children of the clest of the children of israel, and the head of the lord god say the plague of the lord.\n",
      "\n",
      "25:32 and he said, the son of his son one an atonement for the congregation, and was the court years, and for the lord years of the midst \n",
      "------ temperature: 1.0\n",
      "nd was the court years, and for the lord years of the midst of jephep.\n",
      "\n",
      "10:8 and they shall ean his nine, the begepons of the robranam in the land be a\n",
      "rriat? for iw chorgou, yool that hath sept the people frokef down for the lewitess of\n",
      "thy feish to yim\n",
      "see that sasta, and put of her offering, or the not low\n",
      "came so\n",
      "sheupeed his father; for so mel to hurlly his witter and a\n",
      "corn of shem, he gave before her\n",
      "fifty to god, and cruth unto a lond broeken ustoo\n",
      "------ temperature: 1.2\n",
      "before her\n",
      "fifty to god, and cruth unto a lond broeken ustooked, and a aacount\n",
      "vareto very\n",
      "master, ard all became to every rne: locke placey, the altar in all pharaven purphars wied her; and there i rean gefary,\n",
      "whose have eretuar thebing with them, as be aaron an he ffor them ixrwe ;, i have lot pharaoh , dwend, reward\n",
      "a cour of, thus shalt how twented all the name vered in onefar.\n",
      "\n",
      "ealse in serdon, and shepaid\n",
      "for\n",
      "not.\n",
      "\n",
      "\n",
      "9:21 and i will thire he hast the\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "199980/199980 [==============================] - 299s 1ms/step - loss: 1.1710\n",
      "--- Generating with seed: \"from thence; that we may live, and not die.\n",
      "\n",
      "42:3 and joseph\"\n",
      "------ temperature: 0.2\n",
      "from thence; that we may live, and not die.\n",
      "\n",
      "42:3 and joseph said, the lord spake unto moses, saying, 13:12 and thou shalt be a stranger, and the tabernacle of the seven hand, and the lord spake unto moses, saying, 11:11 and the lord spake unto moses, saying, and the children of israel shall be a stranger, and the land of the seven hand and a sin offering before the lord.\n",
      "\n",
      "25:17 and thou shalt be a stranger, and the tabernacle of the children of israel, an\n",
      "------ temperature: 0.5\n",
      "a stranger, and the tabernacle of the children of israel, and shall be man shall be unclean.\n",
      "\n",
      "37:11 and the priest shall do go them there\n",
      "be an offering shall be a stranger, and the seven wood shall be blessed it with the men offering from them from his servants, and shall be went out in the land in the land of esau, and the heavens be\n",
      "blessed out the water, and of the seven wood shall shall be a camels and that were twenty shall be shall be all the strang\n",
      "------ temperature: 1.0\n",
      "camels and that were twenty shall be shall be all the stranger,\n",
      "nor thy people, herony four do in the egyptians; for all that seveiterve the names go, sevet\n",
      "i godd, and\n",
      "thou shalt make the house rosed, and multiply.\n",
      "\n",
      "11:17 then ye shall come year set\n",
      "in aster worl i am\n",
      "consen9ingeneon,\n",
      "and seven him shall be cuttle of every yeay wood his die, and be against from\n",
      "the one wank wintling into every\n",
      "man out of the children of way to keephish\n",
      "after an offering b\n",
      "------ temperature: 1.2\n",
      "n out of the children of way to keephish\n",
      "after an offering becastehcath every night fordy fathers, and denam\n",
      "thereinto the bring\n",
      "by minag.\n",
      "\n",
      "14:16 and if aaron from machcest  a do yeassnegy: asted harden in all\n",
      "nusilhe\n",
      "nestomat until a\n",
      "cugation: then greats cattle after thowes t out\n",
      "hory man seovereth and begaust, thou sheaked ye\n",
      "shall not ye round as to seven 's us, by the even, when accome unto dounyed but\n",
      "there boind all that, senn it: it allah his sen\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "#for epoch in range(1, 60):\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible. With a high temperature, the local structure starts breaking down and most words look like semi-random strings of characters. A clever balance between learned structure and randomness is what makes generation interesting.\n",
    "\n",
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
